{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4349004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7f4eda",
   "metadata": {},
   "source": [
    "**Text lowercase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c1b558c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weather is too cloudy.possiblity of rain is high,today!!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lowercase_text(text): \n",
    "    return text.lower() \n",
    "  \n",
    "input_str = \"Weather is too Cloudy.Possiblity of Rain is High,Today!!\"\n",
    "lowercase_text(input_str) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b88988",
   "metadata": {},
   "source": [
    "**Remove numbers**\n",
    "\n",
    "We should either remove the numbers or convert those numbers into textual representations. We use regular expressions(re) to remove the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "728a83b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You bought  candies from shop, and  candies are in home.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Removing numbers \n",
    "def remove_num(text): \n",
    "    result = re.sub(r'\\d+', '', text)  # d is for digit\n",
    "    return result \n",
    "  \n",
    "input_s = \"You bought 6 candies from shop, and 4 candies are in home.\"\n",
    "remove_num(input_s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5de8cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting inflectNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading inflect-6.0.2-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: pydantic>=1.9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from inflect) (1.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic>=1.9.1->inflect) (4.3.0)\n",
      "Installing collected packages: inflect\n",
      "Successfully installed inflect-6.0.2\n"
     ]
    }
   ],
   "source": [
    "pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7937d8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You bought six candies from shop, and four candies are in home.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert the numbers into words\n",
    "# import the library \n",
    "import inflect \n",
    "q = inflect.engine() \n",
    "  \n",
    "# convert number into text \n",
    "def convert_num(text): \n",
    "    # split strings into list of texts \n",
    "    temp_string = text.split() \n",
    "    # initialise empty list \n",
    "    new_str = [] \n",
    "  \n",
    "    for word in temp_string: \n",
    "        # if text is a digit, convert the digit \n",
    "        # to numbers and append into the new_str list \n",
    "        if word.isdigit(): \n",
    "            temp = q.number_to_words(word) \n",
    "            new_str.append(temp) \n",
    "  \n",
    "        # append the texts as it is \n",
    "        else: \n",
    "            new_str.append(word) \n",
    "  \n",
    "    # join the texts of new_str to form a string \n",
    "    temp_str = ' '.join(new_str) \n",
    "    return temp_str \n",
    "  \n",
    "input_str = 'You bought 6 candies from shop, and 4 candies are in home.'\n",
    "convert_num(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4f069c",
   "metadata": {},
   "source": [
    "**Remove Punctuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "059a416b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ae97bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey Are you excited After a week we will be in Shimla'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's remove punctuation \n",
    "def rem_punct(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator) \n",
    "  \n",
    "input_str = \"Hey, Are you excited??, After a week, we will be in Shimla!!!\"\n",
    "rem_punct(input_str) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba063e74",
   "metadata": {},
   "source": [
    "**Remove default stopwords:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400ce0c6",
   "metadata": {},
   "source": [
    "Stopwords are words that do not contribute to the meaning of the sentence. Hence, they can be safely removed without causing any change in the meaning of a sentence. The NLTK(Natural Language Toolkit) library has the set of stopwords and we can use these to remove stopwords from our text and return a list of word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4274fc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Data', 'new', 'oil', '.', 'A.I', 'last', 'invention']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing nltk library\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "  \n",
    "# remove stopwords function \n",
    "def rem_stopwords(text): \n",
    "    stop_words = set(stopwords.words(\"english\")) \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
    "    return filtered_text \n",
    "  \n",
    "ex_text = \"Data is the new oil. A.I is the last invention\"\n",
    "rem_stopwords(ex_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3773c9c",
   "metadata": {},
   "source": [
    "**Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18d89d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'is',\n",
       " 'the',\n",
       " 'new',\n",
       " 'revolut',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'day',\n",
       " 'one',\n",
       " 'individu',\n",
       " 'would',\n",
       " 'gener',\n",
       " 'terabyt',\n",
       " 'of',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing nltk's porter stemmer \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "stem1 = PorterStemmer() \n",
    "  \n",
    "# stem words in the list of tokenised words \n",
    "def s_words(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    stems = [stem1.stem(word) for word in word_tokens] \n",
    "    return stems \n",
    "  \n",
    "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
    "s_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81811531",
   "metadata": {},
   "source": [
    "**Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "865529f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'be',\n",
       " 'the',\n",
       " 'new',\n",
       " 'revolution',\n",
       " 'in',\n",
       " 'the',\n",
       " 'World',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'day',\n",
       " 'one',\n",
       " 'individual',\n",
       " 'would',\n",
       " 'generate',\n",
       " 'terabytes',\n",
       " 'of',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import wordnet \n",
    "from nltk.tokenize import word_tokenize \n",
    "lemma = wordnet.WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "# lemmatize string \n",
    "def lemmatize_word(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    # provide context i.e. part-of-speech(pos)\n",
    "    lemmas = [lemma.lemmatize(word, pos ='v') for word in word_tokens] \n",
    "    return lemmas \n",
    "  \n",
    "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
    "lemmatize_word(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579b6ac8",
   "metadata": {},
   "source": [
    "**Parts of Speech (POS) Tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2dc35af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "701dd9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Are', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('afraid', 'IN'),\n",
       " ('of', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing tokenize library\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "  \n",
    "# convert text into word_tokens with their tags \n",
    "def pos_tagg(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    return pos_tag(word_tokens) \n",
    "  \n",
    "pos_tagg('Are you afraid of something?') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37932c12",
   "metadata": {},
   "source": [
    "In the above example NNP stands for Proper noun, PRP stands for personal noun, IN as Preposition. We can get all the details pos tags using the Penn Treebank tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a93d7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping help\\tagsets.zip.\n"
     ]
    }
   ],
   "source": [
    "# downloading the tagset  \n",
    "nltk.download('tagsets') \n",
    "  \n",
    "# extract information about the tag \n",
    "nltk.help.upenn_tagset('PRP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c79b88f",
   "metadata": {},
   "source": [
    "**Chunking**\n",
    "\n",
    "\n",
    "Chunking is the process of extracting phrases from the Unstructured text and give them more structure to it. We also called them shallow parsing.We can do it on top of pos tagging. It groups words into chunks mainly for noun phrases. chunking we do by using regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4d971ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ red/JJ parrot/NN)\n",
      "  is/VBZ\n",
      "  flying/VBG\n",
      "  in/IN\n",
      "  (NP the/DT sky/NN))\n",
      "(NP the/DT little/JJ red/JJ parrot/NN)\n",
      "(NP the/DT sky/NN)\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk import pos_tag \n",
    "  \n",
    "# here we define chunking function with text and regular \n",
    "# expressions representing grammar as parameter \n",
    "def chunking(text, grammar): \n",
    "    word_tokens = word_tokenize(text) \n",
    "  \n",
    "    # label words with pos \n",
    "    word_pos = pos_tag(word_tokens) \n",
    "  \n",
    "    # create chunk parser using grammar \n",
    "    chunkParser = nltk.RegexpParser(grammar) \n",
    "  \n",
    "    # test it on the list of word tokens with tagged pos \n",
    "    tree = chunkParser.parse(word_pos) \n",
    "      \n",
    "    for subtree in tree.subtrees(): \n",
    "        print(subtree) \n",
    "    #tree.draw() \n",
    "      \n",
    "sentence = 'the little red parrot is flying in the sky'\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunking(sentence, grammar) "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "f0314d23",
   "metadata": {},
   "source": [
    "In the above example, we defined the grammar by using the regular expression rule. This rule tells you that NP(noun phrase) chunk should be formed whenever the chunker find the optional determiner(DJ) followed by any no. of adjectives and then a NN(noun).\n",
    "\n",
    "Image after running above code.\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Libraries like Spacy and TextBlob are best for chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0329a674",
   "metadata": {},
   "source": [
    "**Named Entity Recognition**\n",
    "\n",
    "It is used to extract information from unstructured text. It is used to classy the entities which is present in the text into categories like a person, organization, event, places, etc. This will give you a detail knowledge about the text and the relationship between the different entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e287ac0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    }
   ],
   "source": [
    "#Importing tokenization and chunk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag, ne_chunk \n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "  \n",
    "def ner(text): \n",
    "    # tokenize the text \n",
    "    word_tokens = word_tokenize(text) \n",
    "  \n",
    "    # pos tagging of words \n",
    "    word_pos = pos_tag(word_tokens) \n",
    "  \n",
    "    # tree of word entities \n",
    "    print(ne_chunk(word_pos)) \n",
    "  \n",
    "text = 'Brain Lara scored the highest 400 runs in a test match which played in between WI and England.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f34f9b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Brain/NNP)\n",
      "  (PERSON Lara/NNP)\n",
      "  scored/VBD\n",
      "  the/DT\n",
      "  highest/JJS\n",
      "  400/CD\n",
      "  runs/NNS\n",
      "  in/IN\n",
      "  a/DT\n",
      "  test/NN\n",
      "  match/NN\n",
      "  which/WDT\n",
      "  played/VBD\n",
      "  in/IN\n",
      "  between/IN\n",
      "  (ORGANIZATION WI/NNP)\n",
      "  and/CC\n",
      "  (GPE England/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "ner(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92346350",
   "metadata": {},
   "source": [
    "**spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360a7975",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.cli.download(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8403c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN dobj\n",
      "startup NOUN dep\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "# https://spacy.io/usage/linguistic-features\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # en_core_web_sm is a corpus based on that grammer is written\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n",
    "\n",
    "# Text: The original word text.\n",
    "# Lemma: The base form of the word.\n",
    "# POS: The simple part-of-speech tag.\n",
    "# Tag: The detailed part-of-speech tag.\n",
    "# Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "# Shape: The word shape – capitalization, punctuation, digits.\n",
    "# is alpha: Is the token an alpha character?\n",
    "# is stop: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557beef",
   "metadata": {},
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077b5474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2fc39e",
   "metadata": {},
   "source": [
    "**Part-of-speech(pos) tags and dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad993af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coronavirus coronavirus NOUN NN nsubj Xxxxx True False\n",
      ": : PUNCT : punct : False False\n",
      "Delhi Delhi PROPN NNP advmod Xxxxx True False\n",
      "resident resident NOUN NN compound xxxx True False\n",
      "tests test NOUN NNS nsubj xxxx True False\n",
      "positive positive ADJ JJ amod xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "coronavirus coronavirus NOUN NN pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "total total ADJ JJ ROOT xxxx True False\n",
      "31 31 NUM CD nummod dd False False\n",
      "people people NOUN NNS dobj xxxx True False\n",
      "infected infect VERB VBN acl xxxx True False\n",
      "in in ADP IN prep xx True True\n",
      "India India PROPN NNP pobj Xxxxx True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Coronavirus: Delhi resident tests positive for coronavirus, total 31 people infected in India\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e1d12e",
   "metadata": {},
   "source": [
    "**word Dependency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a029ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\all\\lib\\site-packages\\spacy\\displacy\\__init__.py:108: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"c4162f2d34564030b38e03975fc3ee95-0\" class=\"displacy\" width=\"1450\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Google,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">crack</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">down</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">fake</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">coronavirus</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">apps</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4162f2d34564030b38e03975fc3ee95-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,89.5 395.0,89.5 395.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4162f2d34564030b38e03975fc3ee95-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4162f2d34564030b38e03975fc3ee95-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4162f2d34564030b38e03975fc3ee95-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4162f2d34564030b38e03975fc3ee95-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4162f2d34564030b38e03975fc3ee95-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prt</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4162f2d34564030b38e03975fc3ee95-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4162f2d34564030b38e03975fc3ee95-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,266.5 L753.0,254.5 737.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4162f2d34564030b38e03975fc3ee95-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,89.5 1270.0,89.5 1270.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4162f2d34564030b38e03975fc3ee95-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4162f2d34564030b38e03975fc3ee95-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4162f2d34564030b38e03975fc3ee95-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4162f2d34564030b38e03975fc3ee95-0-6\" stroke-width=\"2px\" d=\"M770,264.5 C770,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4162f2d34564030b38e03975fc3ee95-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,266.5 L1283.0,254.5 1267.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Google, Apple crack down on fake coronavirus apps\")\n",
    "displacy.serve(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53adcd2",
   "metadata": {},
   "source": [
    "**Ner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e27140c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delhi 13 18 GPE\n",
      "31 66 68 CARDINAL\n",
      "India 88 93 GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Coronavirus: Delhi resident tests positive for coronavirus, total 31 people infected in India\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa928d9",
   "metadata": {},
   "source": [
    "**Visualizing the Named Entity recognizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "069441d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Coronavirus: \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Delhi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " resident tests positive for coronavirus, total \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    31\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " people infected in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    India\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "text = \"Coronavirus: Delhi resident tests positive for coronavirus, total 31 people infected in India\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "displacy.serve(doc, style=\"ent\")\n",
    "# https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ed911e",
   "metadata": {},
   "source": [
    "**Words vector and similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bde42838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download en_core_web_md\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_md\")\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3951271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lion True 55.145737 False\n",
      "bear True 52.114674 False\n",
      "apple True 43.366478 False\n",
      "banana True 31.620354 False\n",
      "fadsfdshds False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "tokens = nlp(\"lion bear apple banana fadsfdshds\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
    "# token.has_vector meaning that token is a part of spacy library or not    \n",
    "# Vector norm: The L2 norm of the token’s vector (the square root of the sum of the values squared)\n",
    "# has vector: Does the token have a vector representation?\n",
    "# OOV: Out-of-vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e80ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c3a61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lion lion 1.0\n",
      "lion bear 0.40031397342681885\n",
      "lion cow 0.4524093568325043\n",
      "lion apple 0.06742795556783676\n",
      "lion mango 0.18510110676288605\n",
      "lion spinach 0.06951922923326492\n",
      "bear lion 0.40031397342681885\n",
      "bear bear 1.0\n",
      "bear cow 0.2781473696231842\n",
      "bear apple 0.18584337830543518\n",
      "bear mango 0.14443379640579224\n",
      "bear spinach 0.0758492723107338\n",
      "cow lion 0.4524093568325043\n",
      "cow bear 0.2781473696231842\n",
      "cow cow 1.0\n",
      "cow apple 0.2575658857822418\n",
      "cow mango 0.26287969946861267\n",
      "cow spinach 0.261837899684906\n",
      "apple lion 0.06742795556783676\n",
      "apple bear 0.18584337830543518\n",
      "apple cow 0.2575658857822418\n",
      "apple apple 1.0\n",
      "apple mango 0.6305076479911804\n",
      "apple spinach 0.5129707455635071\n",
      "mango lion 0.18510110676288605\n",
      "mango bear 0.14443379640579224\n",
      "mango cow 0.26287969946861267\n",
      "mango apple 0.6305076479911804\n",
      "mango mango 1.0\n",
      "mango spinach 0.5483009219169617\n",
      "spinach lion 0.06951922923326492\n",
      "spinach bear 0.0758492723107338\n",
      "spinach cow 0.261837899684906\n",
      "spinach apple 0.5129707455635071\n",
      "spinach mango 0.5483009219169617\n",
      "spinach spinach 1.0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")  # make sure to use larger model!\n",
    "tokens = nlp(\"lion bear cow apple mango spinach\")\n",
    "\n",
    "for token11 in tokens:\n",
    "    for token13 in tokens:\n",
    "        print(token11.text, token13.text, token11.similarity(token13))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

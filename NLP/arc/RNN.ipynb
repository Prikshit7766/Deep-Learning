{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How integer encode using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['recurrent neural network',\n",
    "\t\t'neural network',\n",
    "\t\t'artificial neural',\n",
    "\t\t'connections between nodes',\n",
    "\t\t'can create a cycle',\n",
    "\t\t'allowing output',\n",
    "\t\t'some nodes to affect subsequent',\n",
    "\t\t'exhibit temporal',\n",
    "\t\t'dynamic behavior',\n",
    "\t\t'type of Neural Network',\n",
    "    'affect subsequent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(oov_token='<nothing>')  # oov: out of vocabulary means unknown words in test data will be replaced by <nothing>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<nothing>': 1,\n",
       " 'neural': 2,\n",
       " 'network': 3,\n",
       " 'nodes': 4,\n",
       " 'affect': 5,\n",
       " 'subsequent': 6,\n",
       " 'recurrent': 7,\n",
       " 'artificial': 8,\n",
       " 'connections': 9,\n",
       " 'between': 10,\n",
       " 'can': 11,\n",
       " 'create': 12,\n",
       " 'a': 13,\n",
       " 'cycle': 14,\n",
       " 'allowing': 15,\n",
       " 'output': 16,\n",
       " 'some': 17,\n",
       " 'to': 18,\n",
       " 'exhibit': 19,\n",
       " 'temporal': 20,\n",
       " 'dynamic': 21,\n",
       " 'behavior': 22,\n",
       " 'type': 23,\n",
       " 'of': 24}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index # word_index is a dictionary of words and their uniquely assigned integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('recurrent', 1),\n",
       "             ('neural', 4),\n",
       "             ('network', 3),\n",
       "             ('artificial', 1),\n",
       "             ('connections', 1),\n",
       "             ('between', 1),\n",
       "             ('nodes', 2),\n",
       "             ('can', 1),\n",
       "             ('create', 1),\n",
       "             ('a', 1),\n",
       "             ('cycle', 1),\n",
       "             ('allowing', 1),\n",
       "             ('output', 1),\n",
       "             ('some', 1),\n",
       "             ('to', 1),\n",
       "             ('affect', 2),\n",
       "             ('subsequent', 2),\n",
       "             ('exhibit', 1),\n",
       "             ('temporal', 1),\n",
       "             ('dynamic', 1),\n",
       "             ('behavior', 1),\n",
       "             ('type', 1),\n",
       "             ('of', 1)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts # word_counts is a dictionary of words and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.document_count # document_count is the number of documents that were used to fit the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 2, 3],\n",
       " [2, 3],\n",
       " [8, 2],\n",
       " [9, 10, 4],\n",
       " [11, 12, 13, 14],\n",
       " [15, 16],\n",
       " [17, 4, 18, 5, 6],\n",
       " [19, 20],\n",
       " [21, 22],\n",
       " [23, 24, 2, 3],\n",
       " [5, 6]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(docs) # texts_to_sequences() converts a list of texts to a list of sequences of integers\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "padded_sequences = pad_sequences(sequences, padding='post') # pad_sequences() transforms a list of sequences into a 2D Numpy array of shape (num_samples, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  2,  3,  0,  0],\n",
       "       [ 2,  3,  0,  0,  0],\n",
       "       [ 8,  2,  0,  0,  0],\n",
       "       [ 9, 10,  4,  0,  0],\n",
       "       [11, 12, 13, 14,  0],\n",
       "       [15, 16,  0,  0,  0],\n",
       "       [17,  4, 18,  5,  6],\n",
       "       [19, 20,  0,  0,  0],\n",
       "       [21, 22,  0,  0,  0],\n",
       "       [23, 24,  2,  3,  0],\n",
       "       [ 5,  6,  0,  0,  0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this integer approch is not a good idea because it does not take into account the semantic meaning of words \n",
    "# as we have to add padding to make all the sequences of same length which make the computation expensive\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten , Dropout,SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0] # each review is a list of integers where each integer represents a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n",
      "189\n"
     ]
    }
   ],
   "source": [
    "# problem : length of each review is different\n",
    "print(len(x_train[0]))\n",
    "print(len(x_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "# To solve this problem we will use padding\n",
    "x_train = pad_sequences(x_train, maxlen=250) # maxlen is the maximum length of each review\n",
    "x_test = pad_sequences(x_test, maxlen=250) # maxlen is the maximum length of each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train[0]))\n",
    "print(len(x_train[1]))\n",
    "# now all the reviews are of same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 32)                1088      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,121\n",
      "Trainable params: 1,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model building\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(32, input_shape=(250,1),return_sequences=False))  # here 250 times unrolled\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the meaning of `return_sequences=False`. In an RNN layer, the `return_sequences` parameter determines whether the layer should return the output sequence for each input time step or just the final output.\n",
    "\n",
    "When `return_sequences=False`, the RNN layer only returns the final output for the last time step of the input sequence. This is suitable for many-to-one architectures because we are only interested in the final prediction or output for the entire sequence. In your code, the `SimpleRNN` layer has `return_sequences=False`, indicating that it will only return the final output.\n",
    "\n",
    "If you were to set `return_sequences=True`, the RNN layer would return the output for each time step in the input sequence, resulting in a sequence of outputs. This is useful for many-to-many architectures, where the output of each time step is important, such as in sequence generation tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the `SimpleRNN` layer declaration you mentioned: `SimpleRNN(32, input_shape=(250,1))`.\n",
    "\n",
    "- `32`: This specifies the number of units or cells in the SimpleRNN layer. These units represent the internal computational elements of the layer that process the input sequence.\n",
    "- `input_shape=(250,1)`: This defines the shape of the input data that will be fed into the SimpleRNN layer. It indicates that the input sequences will have a length of 250 time steps, and each time step will have a single feature.\n",
    "In the case of the `SimpleRNN` layer, the number of units (32 in this case) determines the complexity and representational capacity of the layer. It influences the layer's ability to learn and model patterns in the input sequence.\n",
    "\n",
    "On the other hand, the input shape (250, 1) defines the shape of the input data that will be processed by the `SimpleRNN` layer. It specifies that the input sequences will have a length of 250 time steps, and each time step will have a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 39s 242ms/step - loss: 0.7026 - acc: 0.5016 - val_loss: 0.6930 - val_acc: 0.5112\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 39s 249ms/step - loss: 0.6926 - acc: 0.5117 - val_loss: 0.6926 - val_acc: 0.5072\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 41s 262ms/step - loss: 0.6926 - acc: 0.5118 - val_loss: 0.6933 - val_acc: 0.5048\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 38s 242ms/step - loss: 0.6930 - acc: 0.5076 - val_loss: 0.6926 - val_acc: 0.5044\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 42s 266ms/step - loss: 0.6926 - acc: 0.5082 - val_loss: 0.6925 - val_acc: 0.5040\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 41s 262ms/step - loss: 0.6925 - acc: 0.5096 - val_loss: 0.6927 - val_acc: 0.5122\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 40s 258ms/step - loss: 0.6925 - acc: 0.5103 - val_loss: 0.6924 - val_acc: 0.5056\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 41s 263ms/step - loss: 0.6927 - acc: 0.5095 - val_loss: 0.6923 - val_acc: 0.5092\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 44s 282ms/step - loss: 0.6925 - acc: 0.5105 - val_loss: 0.6946 - val_acc: 0.5048\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 40s 256ms/step - loss: 0.6924 - acc: 0.5078 - val_loss: 0.6929 - val_acc: 0.5048\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to do encodings using keras embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['recurrent neural network',\n",
    "\t\t'neural network',\n",
    "\t\t'artificial neural',\n",
    "\t\t'connections between nodes',\n",
    "\t\t'can create a cycle',\n",
    "\t\t'allowing output',\n",
    "\t\t'some nodes to affect subsequent',\n",
    "\t\t'exhibit temporal',\n",
    "\t\t'dynamic behavior',\n",
    "\t\t'type of Neural Network',\n",
    "    'affect subsequent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer=Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 1, 2],\n",
       " [1, 2],\n",
       " [7, 1],\n",
       " [8, 9, 3],\n",
       " [10, 11, 12, 13],\n",
       " [14, 15],\n",
       " [16, 3, 17, 4, 5],\n",
       " [18, 19],\n",
       " [20, 21],\n",
       " [22, 23, 1, 2],\n",
       " [4, 5]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(docs)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  1,  2,  0,  0],\n",
       "       [ 1,  2,  0,  0,  0],\n",
       "       [ 7,  1,  0,  0,  0],\n",
       "       [ 8,  9,  3,  0,  0],\n",
       "       [10, 11, 12, 13,  0],\n",
       "       [14, 15,  0,  0,  0],\n",
       "       [16,  3, 17,  4,  5],\n",
       "       [18, 19,  0,  0,  0],\n",
       "       [20, 21,  0,  0,  0],\n",
       "       [22, 23,  1,  2,  0],\n",
       "       [ 4,  5,  0,  0,  0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import pad_sequences\n",
    "sequences = pad_sequences(sequences,padding='post')\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 5, 2)              46        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46\n",
      "Trainable params: 46\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(23,output_dim=2,input_length=5)) #Total vocab len, ouput dim(per word would be represend by 2 vector), input len per row\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\"adam\",\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 142ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.02693019, -0.02901098],\n",
       "        [-0.00583706,  0.03699764],\n",
       "        [ 0.02434461, -0.02477236],\n",
       "        [-0.02572801,  0.01593998],\n",
       "        [-0.02572801,  0.01593998]],\n",
       "\n",
       "       [[-0.00583706,  0.03699764],\n",
       "        [ 0.02434461, -0.02477236],\n",
       "        [-0.02572801,  0.01593998],\n",
       "        [-0.02572801,  0.01593998],\n",
       "        [-0.02572801,  0.01593998]],\n",
       "\n",
       "       [[-0.0209319 ,  0.00192506],\n",
       "        [-0.00583706,  0.03699764],\n",
       "        [-0.02572801,  0.01593998],\n",
       "        [-0.02572801,  0.01593998],\n",
       "        [-0.02572801,  0.01593998]],\n",
       "\n",
       "       [[-0.044108  ,  0.03198164],\n",
       "        [ 0.04077939,  0.02992911],\n",
       "        [ 0.036444  , -0.01542908],\n",
       "        [-0.02572801,  0.01593998],\n",
       "        [-0.02572801,  0.01593998]],\n",
       "\n",
       "       [[ 0.00090504,  0.01098108],\n",
       "        [ 0.00397978,  0.03963007],\n",
       "        [-0.00771817,  0.0484598 ],\n",
       "        [-0.04630901, -0.00688462],\n",
       "        [-0.02572801,  0.01593998]],\n",
       "\n",
       "       [[ 0.00682104, -0.02072327],\n",
       "        [-0.01657227,  0.03277289],\n",
       "        [-0.02572801,  0.01593998],\n",
       "        [-0.02572801,  0.01593998],\n",
       "        [-0.02572801,  0.01593998]],\n",
       "\n",
       "       [[-0.008265  , -0.03994795],\n",
       "        [ 0.036444  , -0.01542908],\n",
       "        [-0.0051093 , -0.00557138],\n",
       "        [-0.04091418,  0.00783923],\n",
       "        [ 0.00488522, -0.00120588]],\n",
       "\n",
       "       [[ 0.01417245, -0.0227912 ],\n",
       "        [ 0.00449277, -0.02325205],\n",
       "        [-0.02572801,  0.01593998],\n",
       "        [-0.02572801,  0.01593998],\n",
       "        [-0.02572801,  0.01593998]],\n",
       "\n",
       "       [[ 0.00114688, -0.03537846],\n",
       "        [ 0.03343563,  0.04414017],\n",
       "        [-0.02572801,  0.01593998],\n",
       "        [-0.02572801,  0.01593998],\n",
       "        [-0.02572801,  0.01593998]],\n",
       "\n",
       "       [[ 0.0454711 ,  0.03842581],\n",
       "        [ 0.        ,  0.        ],\n",
       "        [-0.00583706,  0.03699764],\n",
       "        [ 0.02434461, -0.02477236],\n",
       "        [-0.02572801,  0.01593998]],\n",
       "\n",
       "       [[-0.04091418,  0.00783923],\n",
       "        [ 0.00488522, -0.00120588],\n",
       "        [-0.02572801,  0.01593998],\n",
       "        [-0.02572801,  0.01593998],\n",
       "        [-0.02572801,  0.01593998]]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(sequences)\n",
    "pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using imdb dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense,SimpleRNN,Embedding,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train,padding='post',maxlen=150)\n",
    "X_test = pad_sequences(X_test,padding='post',maxlen=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 150, 2)            30000     \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 32)                1120      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,153\n",
      "Trainable params: 31,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(15000,output_dim=2,input_length=150))\n",
    "model.add(SimpleRNN(32,return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 206s 261ms/step - loss: 0.6940 - acc: 0.5027 - val_loss: 0.6920 - val_acc: 0.5102\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 196s 251ms/step - loss: 0.6803 - acc: 0.5586 - val_loss: 0.6783 - val_acc: 0.5355\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 207s 265ms/step - loss: 0.6270 - acc: 0.6313 - val_loss: 0.6662 - val_acc: 0.5676\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 210s 268ms/step - loss: 0.5816 - acc: 0.6799 - val_loss: 0.6763 - val_acc: 0.5577\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 223s 285ms/step - loss: 0.5542 - acc: 0.6878 - val_loss: 0.6962 - val_acc: 0.5773\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 202s 258ms/step - loss: 0.4957 - acc: 0.7336 - val_loss: 0.7163 - val_acc: 0.5784\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 194s 248ms/step - loss: 0.4046 - acc: 0.8185 - val_loss: 0.5892 - val_acc: 0.7559\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 194s 248ms/step - loss: 0.4372 - acc: 0.7812 - val_loss: 0.5731 - val_acc: 0.7637\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 193s 247ms/step - loss: 0.3674 - acc: 0.8268 - val_loss: 0.6000 - val_acc: 0.7891\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 195s 249ms/step - loss: 0.4409 - acc: 0.7905 - val_loss: 0.7009 - val_acc: 0.6699\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,epochs=10,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0][0:150].reshape(1,-1).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=x_test[0][0:150].reshape(1,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 269ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.646185]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
